{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /opt/conda/lib/python3.7/site-packages (19.0.3)\n",
      "Requirement already satisfied: pymongo in /opt/conda/lib/python3.7/site-packages (3.7.2)\n",
      "Requirement already up-to-date: nltk in /opt/conda/lib/python3.7/site-packages (3.4)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: singledispatch in /opt/conda/lib/python3.7/site-packages (from nltk) (3.4.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (0.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.16.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting scikit-learn (from sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/cc/a84e1748a2a70d0f3e081f56cefc634f3b57013b16faa6926d3a6f0598df/scikit_learn-0.20.3-cp37-cp37m-manylinux1_x86_64.whl (5.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.4MB 3.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.16.2)\n",
      "Collecting scipy>=0.13.3 (from scikit-learn->sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/7e/5cee36eee5b3194687232f6150a89a38f784883c612db7f4da2ab190980d/scipy-1.2.1-cp37-cp37m-manylinux1_x86_64.whl (24.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.8MB 652kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: scipy, scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-0.20.3 scipy-1.2.1 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pymongo\n",
    "!pip install -U nltk\n",
    "!pip install joblib\n",
    "!pip install pandas\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump, load\n",
    "\n",
    "#Download do corpus da nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _connect_mongo(host, port, username, password, db):\n",
    "    \"\"\" A util for making a connection to mongo \"\"\"\n",
    "\n",
    "    if username and password:\n",
    "        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)\n",
    "        conn = MongoClient(mongo_uri)\n",
    "    else:\n",
    "        conn = MongoClient(host, port)\n",
    "\n",
    "\n",
    "    return conn[db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mongo(db, collection, query={}, host='ds249824.mlab.com', port='49824', username='app', password='nodeapp01', no_id=True):\n",
    "    \"\"\" Read from Mongo and Store into DataFrame \"\"\"\n",
    "\n",
    "    # Connect to MongoDB\n",
    "    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n",
    "\n",
    "    # Make a query to the specific DB and Collection\n",
    "    cursor = db[collection].find(query)\n",
    "\n",
    "    # Expand the cursor and construct the DataFrame\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "\n",
    "    # Delete the _id\n",
    "    if no_id:\n",
    "        del df['_id']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coordinates</th>\n",
       "      <th>data</th>\n",
       "      <th>geo</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>labeled_by</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>place</th>\n",
       "      <th>quoted</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>texto</th>\n",
       "      <th>urls</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>usuario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Tue Feb 19 08:06:49 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>1097769458460954624</td>\n",
       "      <td>Sim</td>\n",
       "      <td>5c7e98f8d363db2425775f24</td>\n",
       "      <td>pt</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>As faixas foram liberadas na Rod Dom Pedro I (...</td>\n",
       "      <td>[{'url': 'https://t.co/IpIWplGWqp', 'expanded_...</td>\n",
       "      <td>[]</td>\n",
       "      <td>radiotransitofm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>Tue Feb 19 08:10:15 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>1097770324278546432</td>\n",
       "      <td>Sim</td>\n",
       "      <td>5c7e98f8d363db2425775f24</td>\n",
       "      <td>pt</td>\n",
       "      <td>São Paulo/SP, Brasil</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Um dos caminhões e a carreta tombaram no acide...</td>\n",
       "      <td>[{'url': 'https://t.co/bgvms9jsTK', 'expanded_...</td>\n",
       "      <td>[]</td>\n",
       "      <td>TransitoSampaSP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>Thu Feb 21 09:11:13 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'text': 'acidente', 'indices': [17, 26]}]</td>\n",
       "      <td>1098510441767161856</td>\n",
       "      <td>Sim</td>\n",
       "      <td>5c7e98f8d363db2425775f24</td>\n",
       "      <td>pt</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Ouvintes relatam #acidente no km 39 da rodovia...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>radiotransitofm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>Wed Feb 20 18:45:23 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>1098292549234671616</td>\n",
       "      <td>Sim</td>\n",
       "      <td>5c7e98f8d363db2425775f24</td>\n",
       "      <td>pt</td>\n",
       "      <td>São Paulo/SP - Brasil</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>13h24 - Acidente de trânsito, carro x moto, na...</td>\n",
       "      <td>[{'url': 'https://t.co/OO7X33NXye', 'expanded_...</td>\n",
       "      <td>[]</td>\n",
       "      <td>BombeirosPMESP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>Wed Feb 20 18:46:15 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>1098292765224566784</td>\n",
       "      <td>Sim</td>\n",
       "      <td>5c7e98f8d363db2425775f24</td>\n",
       "      <td>pt</td>\n",
       "      <td>São Paulo/SP, Brasil</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Acidente de trânsito, carro x moto, na R.Pedro...</td>\n",
       "      <td>[{'url': 'https://t.co/LNXwM9R7WR', 'expanded_...</td>\n",
       "      <td>[{'screen_name': 'BombeirosPMESP', 'name': '19...</td>\n",
       "      <td>TransitoSampaSP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  coordinates                            data   geo  \\\n",
       "0        None  Tue Feb 19 08:06:49 +0000 2019  None   \n",
       "1        None  Tue Feb 19 08:10:15 +0000 2019  None   \n",
       "2        None  Thu Feb 21 09:11:13 +0000 2019  None   \n",
       "3        None  Wed Feb 20 18:45:23 +0000 2019  None   \n",
       "4        None  Wed Feb 20 18:46:15 +0000 2019  None   \n",
       "\n",
       "                                      hashtags                   id label  \\\n",
       "0                                           []  1097769458460954624   Sim   \n",
       "1                                           []  1097770324278546432   Sim   \n",
       "2  [{'text': 'acidente', 'indices': [17, 26]}]  1098510441767161856   Sim   \n",
       "3                                           []  1098292549234671616   Sim   \n",
       "4                                           []  1098292765224566784   Sim   \n",
       "\n",
       "                 labeled_by lang               location place  quoted  \\\n",
       "0  5c7e98f8d363db2425775f24   pt              São Paulo  None   False   \n",
       "1  5c7e98f8d363db2425775f24   pt   São Paulo/SP, Brasil  None   False   \n",
       "2  5c7e98f8d363db2425775f24   pt              São Paulo  None   False   \n",
       "3  5c7e98f8d363db2425775f24   pt  São Paulo/SP - Brasil  None   False   \n",
       "4  5c7e98f8d363db2425775f24   pt   São Paulo/SP, Brasil  None   False   \n",
       "\n",
       "   retweeted                                              texto  \\\n",
       "0      False  As faixas foram liberadas na Rod Dom Pedro I (...   \n",
       "1      False  Um dos caminhões e a carreta tombaram no acide...   \n",
       "2      False  Ouvintes relatam #acidente no km 39 da rodovia...   \n",
       "3      False  13h24 - Acidente de trânsito, carro x moto, na...   \n",
       "4      False  Acidente de trânsito, carro x moto, na R.Pedro...   \n",
       "\n",
       "                                                urls  \\\n",
       "0  [{'url': 'https://t.co/IpIWplGWqp', 'expanded_...   \n",
       "1  [{'url': 'https://t.co/bgvms9jsTK', 'expanded_...   \n",
       "2                                                 []   \n",
       "3  [{'url': 'https://t.co/OO7X33NXye', 'expanded_...   \n",
       "4  [{'url': 'https://t.co/LNXwM9R7WR', 'expanded_...   \n",
       "\n",
       "                                       user_mentions          usuario  \n",
       "0                                                 []  radiotransitofm  \n",
       "1                                                 []  TransitoSampaSP  \n",
       "2                                                 []  radiotransitofm  \n",
       "3                                                 []   BombeirosPMESP  \n",
       "4  [{'screen_name': 'BombeirosPMESP', 'name': '19...  TransitoSampaSP  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acidente = read_mongo('labeling_zone','tweets',query={'label':'Sim'})\n",
    "nao_acidente = read_mongo('labeling_zone','tweets',query={'label':'Não'})\n",
    "#full_data = acidente.append(nao_acidente[0:170])\n",
    "full_data = acidente.append(nao_acidente)\n",
    "\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "1386\n"
     ]
    }
   ],
   "source": [
    "print(len(acidente))\n",
    "print(len(nao_acidente))\n",
    "full_data = acidente.append(nao_acidente[0:263])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "\n",
    "def to_lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['texto_formatado'] = full_data['texto'].apply(lambda t: remove_urls(str(t)))\n",
    "full_data['texto_formatado'] = full_data['texto_formatado'].apply(lambda t: to_lower(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto_formatado</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as faixas foram liberadas na rod dom pedro i (...</td>\n",
       "      <td>[as, faixas, foram, liberadas, na, rod, dom, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>um dos caminhões e a carreta tombaram no acide...</td>\n",
       "      <td>[um, dos, caminhões, e, a, carreta, tombaram, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ouvintes relatam #acidente no km 39 da rodovia...</td>\n",
       "      <td>[ouvintes, relatam, #, acidente, no, km, 39, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13h24 - acidente de trânsito, carro x moto, na...</td>\n",
       "      <td>[13h24, -, acidente, de, trânsito, ,, carro, x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acidente de trânsito, carro x moto, na r.pedro...</td>\n",
       "      <td>[acidente, de, trânsito, ,, carro, x, moto, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     texto_formatado  \\\n",
       "0  as faixas foram liberadas na rod dom pedro i (...   \n",
       "1  um dos caminhões e a carreta tombaram no acide...   \n",
       "2  ouvintes relatam #acidente no km 39 da rodovia...   \n",
       "3  13h24 - acidente de trânsito, carro x moto, na...   \n",
       "4  acidente de trânsito, carro x moto, na r.pedro...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [as, faixas, foram, liberadas, na, rod, dom, p...  \n",
       "1  [um, dos, caminhões, e, a, carreta, tombaram, ...  \n",
       "2  [ouvintes, relatam, #, acidente, no, km, 39, d...  \n",
       "3  [13h24, -, acidente, de, trânsito, ,, carro, x...  \n",
       "4  [acidente, de, trânsito, ,, carro, x, moto, ,,...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "full_data['tokens'] = full_data['texto_formatado'].apply(lambda t: tokenizer.tokenize(t))\n",
    "full_data[['texto_formatado','tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[as, faixas, foram, liberadas, na, rod, dom, p...</td>\n",
       "      <td>[faixas, liberadas, rod, dom, pedro, i, sp-065...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[um, dos, caminhões, e, a, carreta, tombaram, ...</td>\n",
       "      <td>[caminhões, carreta, tombaram, acidente, deixa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ouvintes, relatam, #, acidente, no, km, 39, d...</td>\n",
       "      <td>[ouvintes, relatam, acidente, km, 39, rodovia,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[13h24, -, acidente, de, trânsito, ,, carro, x...</td>\n",
       "      <td>[13h24, acidente, trânsito, carro, x, moto, ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[acidente, de, trânsito, ,, carro, x, moto, ,,...</td>\n",
       "      <td>[acidente, trânsito, carro, x, moto, r.pedro, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[bloqueio, total, do, acesso, do, #, rodoanel,...</td>\n",
       "      <td>[bloqueio, total, acesso, rodoanel, sentido, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ouvintes, relatam, #, acidente, no, km, 39, d...</td>\n",
       "      <td>[ouvintes, relatam, acidente, km, 39, rodovia,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[capotamento, no, #, corredornortesul, sentido...</td>\n",
       "      <td>[capotamento, corredornortesul, sentido, santa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[capotamento, no, #, corredornortesul, sentido...</td>\n",
       "      <td>[capotamento, corredornortesul, sentido, santa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[capotamento, no, #, corredornortesul, sentido...</td>\n",
       "      <td>[capotamento, corredornortesul, sentido, santa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [as, faixas, foram, liberadas, na, rod, dom, p...   \n",
       "1  [um, dos, caminhões, e, a, carreta, tombaram, ...   \n",
       "2  [ouvintes, relatam, #, acidente, no, km, 39, d...   \n",
       "3  [13h24, -, acidente, de, trânsito, ,, carro, x...   \n",
       "4  [acidente, de, trânsito, ,, carro, x, moto, ,,...   \n",
       "5  [bloqueio, total, do, acesso, do, #, rodoanel,...   \n",
       "6  [ouvintes, relatam, #, acidente, no, km, 39, d...   \n",
       "7  [capotamento, no, #, corredornortesul, sentido...   \n",
       "8  [capotamento, no, #, corredornortesul, sentido...   \n",
       "9  [capotamento, no, #, corredornortesul, sentido...   \n",
       "\n",
       "                                               words  \n",
       "0  [faixas, liberadas, rod, dom, pedro, i, sp-065...  \n",
       "1  [caminhões, carreta, tombaram, acidente, deixa...  \n",
       "2  [ouvintes, relatam, acidente, km, 39, rodovia,...  \n",
       "3  [13h24, acidente, trânsito, carro, x, moto, ru...  \n",
       "4  [acidente, trânsito, carro, x, moto, r.pedro, ...  \n",
       "5  [bloqueio, total, acesso, rodoanel, sentido, p...  \n",
       "6  [ouvintes, relatam, acidente, km, 39, rodovia,...  \n",
       "7  [capotamento, corredornortesul, sentido, santa...  \n",
       "8  [capotamento, corredornortesul, sentido, santa...  \n",
       "9  [capotamento, corredornortesul, sentido, santa...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_stopword_list():\n",
    "    portuguese_stops = set(stopwords.words('portuguese'))\n",
    "    portuguese_stops.add('rt')\n",
    "\n",
    "    with open('punctuation.txt','r+') as punct_file:\n",
    "        puncts = punct_file.readlines()\n",
    "\n",
    "    for item in puncts:    \n",
    "        portuguese_stops.add(item.strip())\n",
    "        \n",
    "    return portuguese_stops\n",
    "\n",
    "stop_w = create_stopword_list()\n",
    "\n",
    "#aplica a remocao de stop-words\n",
    "full_data['words'] = full_data['tokens'].apply(lambda w: [word for word in w if word not in stop_w]) \n",
    "#exibe resultado intermediario\n",
    "full_data[['tokens','words']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>stem_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[faixas, liberadas, rod, dom, pedro, i, sp-065...</td>\n",
       "      <td>[faix, liber, rod, dom, pedr, i, sp-065, km, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[caminhões, carreta, tombaram, acidente, deixa...</td>\n",
       "      <td>[caminhõ, carret, tomb, acident, deix, cair, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ouvintes, relatam, acidente, km, 39, rodovia,...</td>\n",
       "      <td>[ouvint, relat, acident, km, 39, rodov, bandei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[13h24, acidente, trânsito, carro, x, moto, ru...</td>\n",
       "      <td>[13h24, acident, trânsit, carr, x, mot, rua, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[acidente, trânsito, carro, x, moto, r.pedro, ...</td>\n",
       "      <td>[acident, trânsit, carr, x, mot, r.pedr, toled...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words  \\\n",
       "0  [faixas, liberadas, rod, dom, pedro, i, sp-065...   \n",
       "1  [caminhões, carreta, tombaram, acidente, deixa...   \n",
       "2  [ouvintes, relatam, acidente, km, 39, rodovia,...   \n",
       "3  [13h24, acidente, trânsito, carro, x, moto, ru...   \n",
       "4  [acidente, trânsito, carro, x, moto, r.pedro, ...   \n",
       "\n",
       "                                          stem_words  \n",
       "0  [faix, liber, rod, dom, pedr, i, sp-065, km, 3...  \n",
       "1  [caminhõ, carret, tomb, acident, deix, cair, p...  \n",
       "2  [ouvint, relat, acident, km, 39, rodov, bandei...  \n",
       "3  [13h24, acident, trânsit, carr, x, mot, rua, p...  \n",
       "4  [acident, trânsit, carr, x, mot, r.pedr, toled...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemer= SnowballStemmer(language='portuguese')\n",
    "full_data['stem_words'] = full_data['words'].apply(lambda t: [stemer.stem(word) for word in t])\n",
    "full_data[['words','stem_words']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    526.000000\n",
       "mean       0.500000\n",
       "std        0.500476\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.500000\n",
       "75%        1.000000\n",
       "max        1.000000\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_label(text):\n",
    "    if text=='Sim':\n",
    "        return 1\n",
    "    elif text=='Não':\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "full_data['target'] = full_data['label'].apply(lambda label: encode_label(label))\n",
    "full_data['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(526, 1936)\n"
     ]
    }
   ],
   "source": [
    "full_data['clean_text'] = full_data['stem_words'].apply(lambda t: str(' '.join(t)))\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_TF = count_vect.fit_transform(full_data['clean_text'])\n",
    "print(X_TF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_TF.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(526, 1936)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X = tfidf_transformer.fit_transform(X_TF)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, full_data['target'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB().fit(X_train, y_train)\n",
    "svm = svm.SVC().fit(X_train,y_train)\n",
    "tree = tree.DecisionTreeClassifier().fit(X_train,y_train)\n",
    "logr = LogisticRegression().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "def print_metrics(y_true,y_predicted):\n",
    "    acc = accuracy_score(y_test.values,y_predicted)\n",
    "    print('Acurácia: ' + str(acc))\n",
    "    #accuracy_score(y_true, y_predicted)\n",
    "    print('Matriz de Confusão:' )\n",
    "    print(confusion_matrix(y_true, y_predicted, labels=[0, 1]))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_true, y_predicted, average='macro')  )\n",
    "    print('Precision')\n",
    "    print(precision_score(y_true, y_predicted, average='macro')  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "Acurácia: 0.8160919540229885\n",
      "Matriz de Confusão:\n",
      "[[72 11]\n",
      " [21 70]]\n",
      "Recall\n",
      "0.8183503243744208\n",
      "Precision\n",
      "0.8191955396256472\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Acurácia: 0.896551724137931\n",
      "Matriz de Confusão:\n",
      "[[76  7]\n",
      " [11 80]]\n",
      "Recall\n",
      "0.8973917648616443\n",
      "Precision\n",
      "0.896551724137931\n",
      "\n",
      "\n",
      "Suport Vector Machines\n",
      "Acurácia: 0.47701149425287354\n",
      "Matriz de Confusão:\n",
      "[[83  0]\n",
      " [91  0]]\n",
      "Recall\n",
      "0.5\n",
      "Precision\n",
      "0.23850574712643677\n",
      "\n",
      "\n",
      "Logistic Regressor\n",
      "Acurácia: 0.9022988505747126\n",
      "Matriz de Confusão:\n",
      "[[77  6]\n",
      " [11 80]]\n",
      "Recall\n",
      "0.9034158612471865\n",
      "Precision\n",
      "0.9026162790697674\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes')\n",
    "print_metrics(y_test,nb.predict(X_test))\n",
    "print('\\n')\n",
    "print('Decision Tree')\n",
    "print_metrics(y_test,tree.predict(X_test))\n",
    "print('\\n')\n",
    "print('Suport Vector Machines')\n",
    "print_metrics(y_test,svm.predict(X_test))\n",
    "print('\\n')\n",
    "print('Logistic Regressor')\n",
    "print_metrics(y_test,logr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
