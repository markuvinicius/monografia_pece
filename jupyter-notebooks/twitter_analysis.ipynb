{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/f3/413bab4ff08e1fc4828dfc59996d721917df8e8583ea85385d51125dceff/pip-19.0.3-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 3.8MB/s \n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 19.0.2\n",
      "    Uninstalling pip-19.0.2:\n",
      "      Successfully uninstalled pip-19.0.2\n",
      "Successfully installed pip-19.0.3\n",
      "Collecting pymongo\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/18/b50834fbfd557eaf07985c2a657b03efc691462ecba62d03e32c2fc4f640/pymongo-3.7.2-cp37-cp37m-manylinux1_x86_64.whl (406kB)\n",
      "\u001b[K    100% |████████████████████████████████| 409kB 1.6MB/s \n",
      "\u001b[?25hInstalling collected packages: pymongo\n",
      "Successfully installed pymongo-3.7.2\n",
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 2.6MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Collecting singledispatch (from nltk)\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/4b/c8/24/b2343664bcceb7147efeb21c0b23703a05b23fcfeaceaa2a1e\n",
      "Successfully built nltk\n",
      "Installing collected packages: singledispatch, nltk\n",
      "Successfully installed nltk-3.4 singledispatch-3.4.0.3\n",
      "Collecting joblib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n",
      "\u001b[K    100% |████████████████████████████████| 286kB 7.0MB/s \n",
      "\u001b[?25hInstalling collected packages: joblib\n",
      "Successfully installed joblib-0.13.2\n",
      "Collecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/e6/2d47835f91eb010036be207581fa113fb4e3822ec1b4bafb0d3d105fede6/pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.1MB 200kB/s \n",
      "\u001b[?25hCollecting pytz>=2011k (from pandas)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/28/1d3920e4d1d50b19bc5d24398a7cd85cc7b9a75a490570d5a30c57622d34/pytz-2018.9-py2.py3-none-any.whl (510kB)\n",
      "\u001b[K    100% |████████████████████████████████| 512kB 2.5MB/s \n",
      "\u001b[?25hCollecting numpy>=1.12.0 (from pandas)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/e7/6c780e612d245cca62bc3ba8e263038f7c144a96a54f877f3714a0e8427e/numpy-1.16.2-cp37-cp37m-manylinux1_x86_64.whl (17.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.3MB 209kB/s \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-1.16.2 pandas-0.24.2 pytz-2018.9\n",
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting scikit-learn (from sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/cc/a84e1748a2a70d0f3e081f56cefc634f3b57013b16faa6926d3a6f0598df/scikit_learn-0.20.3-cp37-cp37m-manylinux1_x86_64.whl (5.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.4MB 260kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.16.2)\n",
      "Collecting scipy>=0.13.3 (from scikit-learn->sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/7e/5cee36eee5b3194687232f6150a89a38f784883c612db7f4da2ab190980d/scipy-1.2.1-cp37-cp37m-manylinux1_x86_64.whl (24.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.8MB 361kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: scipy, scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-0.20.3 scipy-1.2.1 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pymongo\n",
    "!pip install -U nltk\n",
    "!pip install joblib\n",
    "!pip install pandas\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Download do corpus da nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _connect_mongo(host, port, username, password, db):\n",
    "    \"\"\" A util for making a connection to mongo \"\"\"\n",
    "\n",
    "    if username and password:\n",
    "        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)\n",
    "        conn = MongoClient(mongo_uri)\n",
    "    else:\n",
    "        conn = MongoClient(host, port)\n",
    "\n",
    "\n",
    "    return conn[db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mongo(db, collection, query={}, host='ds249824.mlab.com', port='49824', username='app', password='nodeapp01', no_id=True):\n",
    "    \"\"\" Read from Mongo and Store into DataFrame \"\"\"\n",
    "\n",
    "    # Connect to MongoDB\n",
    "    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n",
    "\n",
    "    # Make a query to the specific DB and Collection\n",
    "    cursor = db[collection].find(query)\n",
    "\n",
    "    # Expand the cursor and construct the DataFrame\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "\n",
    "    # Delete the _id\n",
    "    if no_id:\n",
    "        del df['_id']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coordinates</th>\n",
       "      <th>data</th>\n",
       "      <th>geo</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>labeled_by</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>place</th>\n",
       "      <th>quoted</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>texto</th>\n",
       "      <th>urls</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>usuario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Tue Feb 19 08:06:49 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>1097769458460954624</td>\n",
       "      <td>Sim</td>\n",
       "      <td>5c7e98f8d363db2425775f24</td>\n",
       "      <td>pt</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>As faixas foram liberadas na Rod Dom Pedro I (...</td>\n",
       "      <td>[{'url': 'https://t.co/IpIWplGWqp', 'expanded_...</td>\n",
       "      <td>[]</td>\n",
       "      <td>radiotransitofm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>Tue Feb 19 08:10:15 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>1097770324278546432</td>\n",
       "      <td>Sim</td>\n",
       "      <td>5c7e98f8d363db2425775f24</td>\n",
       "      <td>pt</td>\n",
       "      <td>São Paulo/SP, Brasil</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Um dos caminhões e a carreta tombaram no acide...</td>\n",
       "      <td>[{'url': 'https://t.co/bgvms9jsTK', 'expanded_...</td>\n",
       "      <td>[]</td>\n",
       "      <td>TransitoSampaSP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>Thu Feb 21 09:11:13 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'text': 'acidente', 'indices': [17, 26]}]</td>\n",
       "      <td>1098510441767161856</td>\n",
       "      <td>Sim</td>\n",
       "      <td>5c7e98f8d363db2425775f24</td>\n",
       "      <td>pt</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Ouvintes relatam #acidente no km 39 da rodovia...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>radiotransitofm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>Wed Feb 20 18:45:23 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>1098292549234671616</td>\n",
       "      <td>Sim</td>\n",
       "      <td>5c7e98f8d363db2425775f24</td>\n",
       "      <td>pt</td>\n",
       "      <td>São Paulo/SP - Brasil</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>13h24 - Acidente de trânsito, carro x moto, na...</td>\n",
       "      <td>[{'url': 'https://t.co/OO7X33NXye', 'expanded_...</td>\n",
       "      <td>[]</td>\n",
       "      <td>BombeirosPMESP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>Wed Feb 20 18:46:15 +0000 2019</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>1098292765224566784</td>\n",
       "      <td>Sim</td>\n",
       "      <td>5c7e98f8d363db2425775f24</td>\n",
       "      <td>pt</td>\n",
       "      <td>São Paulo/SP, Brasil</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Acidente de trânsito, carro x moto, na R.Pedro...</td>\n",
       "      <td>[{'url': 'https://t.co/LNXwM9R7WR', 'expanded_...</td>\n",
       "      <td>[{'screen_name': 'BombeirosPMESP', 'name': '19...</td>\n",
       "      <td>TransitoSampaSP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  coordinates                            data   geo  \\\n",
       "0        None  Tue Feb 19 08:06:49 +0000 2019  None   \n",
       "1        None  Tue Feb 19 08:10:15 +0000 2019  None   \n",
       "2        None  Thu Feb 21 09:11:13 +0000 2019  None   \n",
       "3        None  Wed Feb 20 18:45:23 +0000 2019  None   \n",
       "4        None  Wed Feb 20 18:46:15 +0000 2019  None   \n",
       "\n",
       "                                      hashtags                   id label  \\\n",
       "0                                           []  1097769458460954624   Sim   \n",
       "1                                           []  1097770324278546432   Sim   \n",
       "2  [{'text': 'acidente', 'indices': [17, 26]}]  1098510441767161856   Sim   \n",
       "3                                           []  1098292549234671616   Sim   \n",
       "4                                           []  1098292765224566784   Sim   \n",
       "\n",
       "                 labeled_by lang               location place  quoted  \\\n",
       "0  5c7e98f8d363db2425775f24   pt              São Paulo  None   False   \n",
       "1  5c7e98f8d363db2425775f24   pt   São Paulo/SP, Brasil  None   False   \n",
       "2  5c7e98f8d363db2425775f24   pt              São Paulo  None   False   \n",
       "3  5c7e98f8d363db2425775f24   pt  São Paulo/SP - Brasil  None   False   \n",
       "4  5c7e98f8d363db2425775f24   pt   São Paulo/SP, Brasil  None   False   \n",
       "\n",
       "   retweeted                                              texto  \\\n",
       "0      False  As faixas foram liberadas na Rod Dom Pedro I (...   \n",
       "1      False  Um dos caminhões e a carreta tombaram no acide...   \n",
       "2      False  Ouvintes relatam #acidente no km 39 da rodovia...   \n",
       "3      False  13h24 - Acidente de trânsito, carro x moto, na...   \n",
       "4      False  Acidente de trânsito, carro x moto, na R.Pedro...   \n",
       "\n",
       "                                                urls  \\\n",
       "0  [{'url': 'https://t.co/IpIWplGWqp', 'expanded_...   \n",
       "1  [{'url': 'https://t.co/bgvms9jsTK', 'expanded_...   \n",
       "2                                                 []   \n",
       "3  [{'url': 'https://t.co/OO7X33NXye', 'expanded_...   \n",
       "4  [{'url': 'https://t.co/LNXwM9R7WR', 'expanded_...   \n",
       "\n",
       "                                       user_mentions          usuario  \n",
       "0                                                 []  radiotransitofm  \n",
       "1                                                 []  TransitoSampaSP  \n",
       "2                                                 []  radiotransitofm  \n",
       "3                                                 []   BombeirosPMESP  \n",
       "4  [{'screen_name': 'BombeirosPMESP', 'name': '19...  TransitoSampaSP  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acidente = read_mongo('labeling_zone','tweets',query={'label':'Sim'})\n",
    "nao_acidente = read_mongo('labeling_zone','tweets',query={'label':'Não'})\n",
    "#full_data = acidente.append(nao_acidente[0:170])\n",
    "full_data = acidente.append(nao_acidente)\n",
    "\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n",
      "1499\n"
     ]
    }
   ],
   "source": [
    "print(len(acidente))\n",
    "print(len(nao_acidente))\n",
    "full_data = acidente.append(nao_acidente[0:277])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "\n",
    "def to_lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['texto_formatado'] = full_data['texto'].apply(lambda t: remove_urls(str(t)))\n",
    "full_data['texto_formatado'] = full_data['texto_formatado'].apply(lambda t: to_lower(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto_formatado</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as faixas foram liberadas na rod dom pedro i (...</td>\n",
       "      <td>[as, faixas, foram, liberadas, na, rod, dom, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>um dos caminhões e a carreta tombaram no acide...</td>\n",
       "      <td>[um, dos, caminhões, e, a, carreta, tombaram, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ouvintes relatam #acidente no km 39 da rodovia...</td>\n",
       "      <td>[ouvintes, relatam, #, acidente, no, km, 39, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13h24 - acidente de trânsito, carro x moto, na...</td>\n",
       "      <td>[13h24, -, acidente, de, trânsito, ,, carro, x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acidente de trânsito, carro x moto, na r.pedro...</td>\n",
       "      <td>[acidente, de, trânsito, ,, carro, x, moto, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     texto_formatado  \\\n",
       "0  as faixas foram liberadas na rod dom pedro i (...   \n",
       "1  um dos caminhões e a carreta tombaram no acide...   \n",
       "2  ouvintes relatam #acidente no km 39 da rodovia...   \n",
       "3  13h24 - acidente de trânsito, carro x moto, na...   \n",
       "4  acidente de trânsito, carro x moto, na r.pedro...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [as, faixas, foram, liberadas, na, rod, dom, p...  \n",
       "1  [um, dos, caminhões, e, a, carreta, tombaram, ...  \n",
       "2  [ouvintes, relatam, #, acidente, no, km, 39, d...  \n",
       "3  [13h24, -, acidente, de, trânsito, ,, carro, x...  \n",
       "4  [acidente, de, trânsito, ,, carro, x, moto, ,,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "full_data['tokens'] = full_data['texto_formatado'].apply(lambda t: tokenizer.tokenize(t))\n",
    "full_data[['texto_formatado','tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[as, faixas, foram, liberadas, na, rod, dom, p...</td>\n",
       "      <td>[faixas, liberadas, rod, dom, pedro, i, sp-065...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[um, dos, caminhões, e, a, carreta, tombaram, ...</td>\n",
       "      <td>[caminhões, carreta, tombaram, acidente, deixa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ouvintes, relatam, #, acidente, no, km, 39, d...</td>\n",
       "      <td>[ouvintes, relatam, acidente, km, 39, rodovia,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[13h24, -, acidente, de, trânsito, ,, carro, x...</td>\n",
       "      <td>[13h24, acidente, trânsito, carro, x, moto, ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[acidente, de, trânsito, ,, carro, x, moto, ,,...</td>\n",
       "      <td>[acidente, trânsito, carro, x, moto, r.pedro, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[bloqueio, total, do, acesso, do, #, rodoanel,...</td>\n",
       "      <td>[bloqueio, total, acesso, rodoanel, sentido, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ouvintes, relatam, #, acidente, no, km, 39, d...</td>\n",
       "      <td>[ouvintes, relatam, acidente, km, 39, rodovia,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[capotamento, no, #, corredornortesul, sentido...</td>\n",
       "      <td>[capotamento, corredornortesul, sentido, santa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[capotamento, no, #, corredornortesul, sentido...</td>\n",
       "      <td>[capotamento, corredornortesul, sentido, santa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[capotamento, no, #, corredornortesul, sentido...</td>\n",
       "      <td>[capotamento, corredornortesul, sentido, santa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [as, faixas, foram, liberadas, na, rod, dom, p...   \n",
       "1  [um, dos, caminhões, e, a, carreta, tombaram, ...   \n",
       "2  [ouvintes, relatam, #, acidente, no, km, 39, d...   \n",
       "3  [13h24, -, acidente, de, trânsito, ,, carro, x...   \n",
       "4  [acidente, de, trânsito, ,, carro, x, moto, ,,...   \n",
       "5  [bloqueio, total, do, acesso, do, #, rodoanel,...   \n",
       "6  [ouvintes, relatam, #, acidente, no, km, 39, d...   \n",
       "7  [capotamento, no, #, corredornortesul, sentido...   \n",
       "8  [capotamento, no, #, corredornortesul, sentido...   \n",
       "9  [capotamento, no, #, corredornortesul, sentido...   \n",
       "\n",
       "                                               words  \n",
       "0  [faixas, liberadas, rod, dom, pedro, i, sp-065...  \n",
       "1  [caminhões, carreta, tombaram, acidente, deixa...  \n",
       "2  [ouvintes, relatam, acidente, km, 39, rodovia,...  \n",
       "3  [13h24, acidente, trânsito, carro, x, moto, ru...  \n",
       "4  [acidente, trânsito, carro, x, moto, r.pedro, ...  \n",
       "5  [bloqueio, total, acesso, rodoanel, sentido, p...  \n",
       "6  [ouvintes, relatam, acidente, km, 39, rodovia,...  \n",
       "7  [capotamento, corredornortesul, sentido, santa...  \n",
       "8  [capotamento, corredornortesul, sentido, santa...  \n",
       "9  [capotamento, corredornortesul, sentido, santa...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_stopword_list():\n",
    "    portuguese_stops = set(stopwords.words('portuguese'))\n",
    "    portuguese_stops.add('rt')\n",
    "\n",
    "    with open('punctuation.txt','r+') as punct_file:\n",
    "        puncts = punct_file.readlines()\n",
    "\n",
    "    for item in puncts:    \n",
    "        portuguese_stops.add(item.strip())\n",
    "        \n",
    "    return portuguese_stops\n",
    "\n",
    "stop_w = create_stopword_list()\n",
    "\n",
    "#aplica a remocao de stop-words\n",
    "full_data['words'] = full_data['tokens'].apply(lambda w: [word for word in w if word not in stop_w]) \n",
    "#exibe resultado intermediario\n",
    "full_data[['tokens','words']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>stem_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[faixas, liberadas, rod, dom, pedro, i, sp-065...</td>\n",
       "      <td>[faix, liber, rod, dom, pedr, i, sp-065, km, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[caminhões, carreta, tombaram, acidente, deixa...</td>\n",
       "      <td>[caminhõ, carret, tomb, acident, deix, cair, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ouvintes, relatam, acidente, km, 39, rodovia,...</td>\n",
       "      <td>[ouvint, relat, acident, km, 39, rodov, bandei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[13h24, acidente, trânsito, carro, x, moto, ru...</td>\n",
       "      <td>[13h24, acident, trânsit, carr, x, mot, rua, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[acidente, trânsito, carro, x, moto, r.pedro, ...</td>\n",
       "      <td>[acident, trânsit, carr, x, mot, r.pedr, toled...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words  \\\n",
       "0  [faixas, liberadas, rod, dom, pedro, i, sp-065...   \n",
       "1  [caminhões, carreta, tombaram, acidente, deixa...   \n",
       "2  [ouvintes, relatam, acidente, km, 39, rodovia,...   \n",
       "3  [13h24, acidente, trânsito, carro, x, moto, ru...   \n",
       "4  [acidente, trânsito, carro, x, moto, r.pedro, ...   \n",
       "\n",
       "                                          stem_words  \n",
       "0  [faix, liber, rod, dom, pedr, i, sp-065, km, 3...  \n",
       "1  [caminhõ, carret, tomb, acident, deix, cair, p...  \n",
       "2  [ouvint, relat, acident, km, 39, rodov, bandei...  \n",
       "3  [13h24, acident, trânsit, carr, x, mot, rua, p...  \n",
       "4  [acident, trânsit, carr, x, mot, r.pedr, toled...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemer= SnowballStemmer(language='portuguese')\n",
    "full_data['stem_words'] = full_data['words'].apply(lambda t: [stemer.stem(word) for word in t])\n",
    "full_data[['words','stem_words']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    554.000000\n",
       "mean       0.500000\n",
       "std        0.500452\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.500000\n",
       "75%        1.000000\n",
       "max        1.000000\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_label(text):\n",
    "    if text=='Sim':\n",
    "        return 1\n",
    "    elif text=='Não':\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "full_data['target'] = full_data['label'].apply(lambda label: encode_label(label))\n",
    "full_data['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(554, 1991)\n"
     ]
    }
   ],
   "source": [
    "full_data['clean_text'] = full_data['stem_words'].apply(lambda t: str(' '.join(t)))\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_TF = count_vect.fit_transform(full_data['clean_text'])\n",
    "print(X_TF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_TF.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(554, 1991)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X = tfidf_transformer.fit_transform(X_TF)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, full_data['target'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB().fit(X_train, y_train)\n",
    "svm = svm.SVC().fit(X_train,y_train)\n",
    "tree = tree.DecisionTreeClassifier().fit(X_train,y_train)\n",
    "logr = LogisticRegression().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc = MultinomialNB()\n",
    "svmc = svm.SVC()\n",
    "treec = tree.DecisionTreeClassifier()\n",
    "logr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Realiza o cross validation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "nb_scores = cross_val_score(nbc, X, full_data['target'], cv=5)\n",
    "svm_scores = cross_val_score(svmc, X, full_data['target'], cv=5)\n",
    "tree_scores = cross_val_score(treec, X, full_data['target'], cv=5)\n",
    "logr_scores = cross_val_score(logr, X, full_data['target'], cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83035714 0.82142857 0.80909091 0.86363636 0.67272727]\n",
      "[0.67857143 0.77678571 0.70909091 0.67272727 0.52727273]\n",
      "[0.89285714 0.83035714 0.86363636 0.9        0.84545455]\n",
      "[0.875      0.875      0.9        0.92727273 0.77272727]\n"
     ]
    }
   ],
   "source": [
    "print(nb_scores)\n",
    "print(svm_scores)\n",
    "print(tree_scores)\n",
    "print(logr_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def print_metrics(y_true,y_predicted):\n",
    "    acc = accuracy_score(y_test.values,y_predicted)\n",
    "    print('Acurácia: ' + str(acc))\n",
    "    #accuracy_score(y_true, y_predicted)\n",
    "    print('Matriz de Confusão:' )\n",
    "    print(confusion_matrix(y_true, y_predicted, labels=[0, 1]))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_true, y_predicted, average='macro')  )\n",
    "    print('Precision')\n",
    "    print(precision_score(y_true, y_predicted, average='macro')  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "Acurácia: 0.825136612021858\n",
      "Matriz de Confusão:\n",
      "[[73 19]\n",
      " [13 78]]\n",
      "Recall\n",
      "0.8253105590062111\n",
      "Precision\n",
      "0.8264804603212659\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Acurácia: 0.8852459016393442\n",
      "Matriz de Confusão:\n",
      "[[81 11]\n",
      " [10 81]]\n",
      "Recall\n",
      "0.8852723363592929\n",
      "Precision\n",
      "0.8852723363592929\n",
      "\n",
      "\n",
      "Suport Vector Machines\n",
      "Acurácia: 0.4972677595628415\n",
      "Matriz de Confusão:\n",
      "[[ 0 92]\n",
      " [ 0 91]]\n",
      "Recall\n",
      "0.5\n",
      "Precision\n",
      "0.24863387978142076\n",
      "\n",
      "\n",
      "Logistic Regressor\n",
      "Acurácia: 0.907103825136612\n",
      "Matriz de Confusão:\n",
      "[[81 11]\n",
      " [ 6 85]]\n",
      "Recall\n",
      "0.9072503583373148\n",
      "Precision\n",
      "0.9082255747126436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes')\n",
    "print_metrics(y_test,nb.predict(X_test))\n",
    "print('\\n')\n",
    "print('Decision Tree')\n",
    "print_metrics(y_test,tree.predict(X_test))\n",
    "print('\\n')\n",
    "print('Suport Vector Machines')\n",
    "print_metrics(y_test,svm.predict(X_test))\n",
    "print('\\n')\n",
    "print('Logistic Regressor')\n",
    "print_metrics(y_test,logr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
