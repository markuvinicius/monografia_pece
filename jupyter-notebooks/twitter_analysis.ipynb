{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /opt/conda/lib/python3.6/site-packages (18.1)\n",
      "Requirement already satisfied: pymongo in /opt/conda/lib/python3.6/site-packages (3.7.2)\n",
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 4.1MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: singledispatch in /opt/conda/lib/python3.6/site-packages (from nltk) (3.4.0.3)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/4b/c8/24/b2343664bcceb7147efeb21c0b23703a05b23fcfeaceaa2a1e\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4\n",
      "Collecting joblib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/d9/4ea194a4c1d0148f9446054b9135f47218c23ccc6f649aeb09fab4c0925c/joblib-0.13.1-py2.py3-none-any.whl (278kB)\n",
      "\u001b[K    100% |████████████████████████████████| 286kB 9.2MB/s \n",
      "\u001b[?25hInstalling collected packages: joblib\n",
      "Successfully installed joblib-0.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pymongo\n",
    "!pip install -U nltk\n",
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump, load\n",
    "\n",
    "#Download do corpus da nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _connect_mongo(host, port, username, password, db):\n",
    "    \"\"\" A util for making a connection to mongo \"\"\"\n",
    "\n",
    "    if username and password:\n",
    "        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)\n",
    "        conn = MongoClient(mongo_uri)\n",
    "    else:\n",
    "        conn = MongoClient(host, port)\n",
    "\n",
    "\n",
    "    return conn[db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mongo(db, collection, query={}, host='ds249824.mlab.com', port='49824', username='app', password='nodeapp01', no_id=True):\n",
    "    \"\"\" Read from Mongo and Store into DataFrame \"\"\"\n",
    "\n",
    "    # Connect to MongoDB\n",
    "    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n",
    "\n",
    "    # Make a query to the specific DB and Collection\n",
    "    cursor = db[collection].find(query)\n",
    "\n",
    "    # Expand the cursor and construct the DataFrame\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "\n",
    "    # Delete the _id\n",
    "    if no_id:\n",
    "        del df['_id']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>labeled_by</th>\n",
       "      <th>texto</th>\n",
       "      <th>usuario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Dec 16 11:27:33 +0000 2018</td>\n",
       "      <td>1074264765747335169</td>\n",
       "      <td>Sim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"\"\"Cidadão de bem\"\"\" confessou ter matado duas...</td>\n",
       "      <td>AIice_Costa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sat Dec 29 10:22:21 +0000 2018</td>\n",
       "      <td>1078959397752266753</td>\n",
       "      <td>Sim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"#INFO #RDRJ temporealnews RT bandnewsfmrio: P...</td>\n",
       "      <td>TempoRealNews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Dec 28 10:22:18 +0000 2018</td>\n",
       "      <td>1078596997848928257</td>\n",
       "      <td>Sim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"#RDRJ RT OperacoesRio: AV. BRASIL | BONSUCESS...</td>\n",
       "      <td>TempoRealNews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thu Dec 27 10:03:12 +0000 2018</td>\n",
       "      <td>1078229806150496256</td>\n",
       "      <td>Sim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"#RDRJ RT OperacoesRio: CAMINHO PARA RODOVIÁRI...</td>\n",
       "      <td>TempoRealNews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat Dec 29 09:52:11 +0000 2018</td>\n",
       "      <td>1078951805688705024</td>\n",
       "      <td>Sim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"#RDRJ RT OperacoesRio: LINHA VERMELHA - Senti...</td>\n",
       "      <td>TempoRealNews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             data                   id label labeled_by  \\\n",
       "0  Sun Dec 16 11:27:33 +0000 2018  1074264765747335169   Sim        NaN   \n",
       "1  Sat Dec 29 10:22:21 +0000 2018  1078959397752266753   Sim        NaN   \n",
       "2  Fri Dec 28 10:22:18 +0000 2018  1078596997848928257   Sim        NaN   \n",
       "3  Thu Dec 27 10:03:12 +0000 2018  1078229806150496256   Sim        NaN   \n",
       "4  Sat Dec 29 09:52:11 +0000 2018  1078951805688705024   Sim        NaN   \n",
       "\n",
       "                                               texto        usuario  \n",
       "0  \"\"\"Cidadão de bem\"\"\" confessou ter matado duas...    AIice_Costa  \n",
       "1  \"#INFO #RDRJ temporealnews RT bandnewsfmrio: P...  TempoRealNews  \n",
       "2  \"#RDRJ RT OperacoesRio: AV. BRASIL | BONSUCESS...  TempoRealNews  \n",
       "3  \"#RDRJ RT OperacoesRio: CAMINHO PARA RODOVIÁRI...  TempoRealNews  \n",
       "4  \"#RDRJ RT OperacoesRio: LINHA VERMELHA - Senti...  TempoRealNews  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acidente = read_mongo('labeling_zone','tweets',query={'label':'Sim'})\n",
    "nao_acidente = read_mongo('labeling_zone','tweets',query={'label':'Não'})\n",
    "full_data = acidente.append(nao_acidente)\n",
    "\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "\n",
    "def to_lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['texto_formatado'] = full_data['texto'].apply(lambda t: remove_urls(str(t)))\n",
    "full_data['texto_formatado'] = full_data['texto_formatado'].apply(lambda t: to_lower(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto_formatado</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"\"\"cidadão de bem\"\"\" confessou ter matado duas...</td>\n",
       "      <td>[``, ``, '', cidadão, de, bem, '', '', '', con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"#info #rdrj temporealnews rt bandnewsfmrio: p...</td>\n",
       "      <td>[``, #, info, #, rdrj, temporealnews, rt, band...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#rdrj rt operacoesrio: av. brasil | bonsucess...</td>\n",
       "      <td>[``, #, rdrj, rt, operacoesrio, :, av., brasil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"#rdrj rt operacoesrio: caminho para rodoviári...</td>\n",
       "      <td>[``, #, rdrj, rt, operacoesrio, :, caminho, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"#rdrj rt operacoesrio: linha vermelha - senti...</td>\n",
       "      <td>[``, #, rdrj, rt, operacoesrio, :, linha, verm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     texto_formatado  \\\n",
       "0  \"\"\"cidadão de bem\"\"\" confessou ter matado duas...   \n",
       "1  \"#info #rdrj temporealnews rt bandnewsfmrio: p...   \n",
       "2  \"#rdrj rt operacoesrio: av. brasil | bonsucess...   \n",
       "3  \"#rdrj rt operacoesrio: caminho para rodoviári...   \n",
       "4  \"#rdrj rt operacoesrio: linha vermelha - senti...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [``, ``, '', cidadão, de, bem, '', '', '', con...  \n",
       "1  [``, #, info, #, rdrj, temporealnews, rt, band...  \n",
       "2  [``, #, rdrj, rt, operacoesrio, :, av., brasil...  \n",
       "3  [``, #, rdrj, rt, operacoesrio, :, caminho, pa...  \n",
       "4  [``, #, rdrj, rt, operacoesrio, :, linha, verm...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "full_data['tokens'] = full_data['texto_formatado'].apply(lambda t: tokenizer.tokenize(t))\n",
    "full_data[['texto_formatado','tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[``, ``, '', cidadão, de, bem, '', '', '', con...</td>\n",
       "      <td>[cidadão, bem, confessou, ter, matado, duas, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[``, #, info, #, rdrj, temporealnews, rt, band...</td>\n",
       "      <td>[info, rdrj, temporealnews, bandnewsfmrio, cau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[``, #, rdrj, rt, operacoesrio, :, av., brasil...</td>\n",
       "      <td>[rdrj, operacoesrio, av., brasil, bonsucesso, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[``, #, rdrj, rt, operacoesrio, :, caminho, pa...</td>\n",
       "      <td>[rdrj, operacoesrio, caminho, rodoviária, trân...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[``, #, rdrj, rt, operacoesrio, :, linha, verm...</td>\n",
       "      <td>[rdrj, operacoesrio, linha, vermelha, sentido,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[``, #, rdrj, rt, operacoesrio, :, tanque, |, ...</td>\n",
       "      <td>[rdrj, operacoesrio, tanque, rua, cândido, ben...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[``, erro, humano, '', na, origem, do, acident...</td>\n",
       "      <td>[erro, humano, origem, acidente, elétrico, 25....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[``, gado, na, pista, provoca, acidente, '', k...</td>\n",
       "      <td>[gado, pista, provoca, acidente, kkkkkkkkkkk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[``, jovem, perde, a, vida, em, grave, acident...</td>\n",
       "      <td>[jovem, perde, vida, grave, acidente, avenida,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[``, manu, ,, n, surta, ,, sofri, um, acidente...</td>\n",
       "      <td>[manu, n, surta, sofri, acidente, moto, quase,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [``, ``, '', cidadão, de, bem, '', '', '', con...   \n",
       "1  [``, #, info, #, rdrj, temporealnews, rt, band...   \n",
       "2  [``, #, rdrj, rt, operacoesrio, :, av., brasil...   \n",
       "3  [``, #, rdrj, rt, operacoesrio, :, caminho, pa...   \n",
       "4  [``, #, rdrj, rt, operacoesrio, :, linha, verm...   \n",
       "5  [``, #, rdrj, rt, operacoesrio, :, tanque, |, ...   \n",
       "6  [``, erro, humano, '', na, origem, do, acident...   \n",
       "7  [``, gado, na, pista, provoca, acidente, '', k...   \n",
       "8  [``, jovem, perde, a, vida, em, grave, acident...   \n",
       "9  [``, manu, ,, n, surta, ,, sofri, um, acidente...   \n",
       "\n",
       "                                               words  \n",
       "0  [cidadão, bem, confessou, ter, matado, duas, t...  \n",
       "1  [info, rdrj, temporealnews, bandnewsfmrio, cau...  \n",
       "2  [rdrj, operacoesrio, av., brasil, bonsucesso, ...  \n",
       "3  [rdrj, operacoesrio, caminho, rodoviária, trân...  \n",
       "4  [rdrj, operacoesrio, linha, vermelha, sentido,...  \n",
       "5  [rdrj, operacoesrio, tanque, rua, cândido, ben...  \n",
       "6  [erro, humano, origem, acidente, elétrico, 25....  \n",
       "7      [gado, pista, provoca, acidente, kkkkkkkkkkk]  \n",
       "8  [jovem, perde, vida, grave, acidente, avenida,...  \n",
       "9  [manu, n, surta, sofri, acidente, moto, quase,...  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_stopword_list():\n",
    "    portuguese_stops = set(stopwords.words('portuguese'))\n",
    "    portuguese_stops.add('rt')\n",
    "\n",
    "    with open('punctuation.txt','r+') as punct_file:\n",
    "        puncts = punct_file.readlines()\n",
    "\n",
    "    for item in puncts:    \n",
    "        portuguese_stops.add(item.strip())\n",
    "        \n",
    "    return portuguese_stops\n",
    "\n",
    "stop_w = create_stopword_list()\n",
    "\n",
    "#aplica a remocao de stop-words\n",
    "full_data['words'] = full_data['tokens'].apply(lambda w: [word for word in w if word not in stop_w]) \n",
    "#exibe resultado intermediario\n",
    "full_data[['tokens','words']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>stem_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[cidadão, bem, confessou, ter, matado, duas, t...</td>\n",
       "      <td>[cidadã, bem, confess, ter, mat, duas, técnic,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[info, rdrj, temporealnews, bandnewsfmrio, cau...</td>\n",
       "      <td>[info, rdrj, temporealnews, bandnewsfmri, caus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[rdrj, operacoesrio, av., brasil, bonsucesso, ...</td>\n",
       "      <td>[rdrj, operacoesri, av., brasil, bonsucess, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[rdrj, operacoesrio, caminho, rodoviária, trân...</td>\n",
       "      <td>[rdrj, operacoesri, caminh, rodoviár, trânsit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[rdrj, operacoesrio, linha, vermelha, sentido,...</td>\n",
       "      <td>[rdrj, operacoesri, linh, vermelh, sent, centr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words  \\\n",
       "0  [cidadão, bem, confessou, ter, matado, duas, t...   \n",
       "1  [info, rdrj, temporealnews, bandnewsfmrio, cau...   \n",
       "2  [rdrj, operacoesrio, av., brasil, bonsucesso, ...   \n",
       "3  [rdrj, operacoesrio, caminho, rodoviária, trân...   \n",
       "4  [rdrj, operacoesrio, linha, vermelha, sentido,...   \n",
       "\n",
       "                                          stem_words  \n",
       "0  [cidadã, bem, confess, ter, mat, duas, técnic,...  \n",
       "1  [info, rdrj, temporealnews, bandnewsfmri, caus...  \n",
       "2  [rdrj, operacoesri, av., brasil, bonsucess, ac...  \n",
       "3  [rdrj, operacoesri, caminh, rodoviár, trânsit,...  \n",
       "4  [rdrj, operacoesri, linh, vermelh, sent, centr...  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemer= SnowballStemmer(language='portuguese')\n",
    "full_data['stem_words'] = full_data['words'].apply(lambda t: [stemer.stem(word) for word in t])\n",
    "full_data[['words','stem_words']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    256.000000\n",
       "mean       0.417969\n",
       "std        0.494191\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        1.000000\n",
       "max        1.000000\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_label(text):\n",
    "    if text=='Sim':\n",
    "        return 1\n",
    "    elif text=='Não':\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "full_data['target'] = full_data['label'].apply(lambda label: encode_label(label))\n",
    "full_data['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1592)\n"
     ]
    }
   ],
   "source": [
    "full_data['clean_text'] = full_data['stem_words'].apply(lambda t: str(' '.join(t)))\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_TF = count_vect.fit_transform(full_data['clean_text'])\n",
    "print(X_TF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01', '027998504942', '040', '050', '085', '101', '106', '10h00', '10ºgbm', '116', '12', '12h15', '12h35', '13h03', '140', '14h02', '15', '150', '15h40', '16', '17', '17h19', '18', '1937', '1ª', '20', '2008', '2018', '2018emfot', '2019', '20h35', '210', '23', '232', '24', '245', '25', '250', '258', '259', '26', '262', '27', '28', '29', '290', '294', '2ª', '30', '35', '376', '381', '386', '3997', '3h', '401', '40h', '447', '470', '50', '525', '535', '603', '61', '614', '64', '667', '67', '676', '70', '75', '751', '76', '826', '91', '_______________________________', '_eco101', 'aaaaaa', 'abaix', 'abaixo', 'abert', 'abord', 'absolut', 'absurd', 'acab', 'acas', 'acertou', 'acess', 'acha', 'acham', 'acho', 'achou', 'acident', 'acidental', 'acidente', 'acidentes', 'acionada', 'acompanh', 'acontec', 'aconteceu', 'acord', 'acredit', 'aeroporto', 'afins', 'agent', 'agor', 'agr', 'aguard', 'aguent', 'agênc', 'agênciagbc', 'ah', 'ai', 'ain', 'aind', 'ajam', 'ajud', 'alag', 'alago', 'alain', 'alan', 'aleatoriedad', 'aleatóri', 'alert', 'algo', 'algum', 'alguns', 'alguém', 'aliment', 'alm', 'alta', 'altur', 'alvo', 'always', 'alça', 'além', 'alô', 'alôuberlând', 'amanda', 'amanhã', 'amazon', 'ambas', 'ambos', 'amig', 'amo', 'amolec', 'amor', 'amér', 'andamento', 'andand', 'andar', 'andrad', 'andreazevedo39', 'android', 'anel', 'angol', 'angra', 'anhangu', 'anim', 'ano', 'anonov', 'anos', 'antes', 'antig', 'antôni', 'aovivomet', 'aparec', 'aparent', 'apen', 'apoi', 'apont', 'aposent', 'app', 'apresent', 'aproxim', 'apéelenãovailong', 'após', 'aq', 'aqu', 'araioses', 'arbítri', 'arquitet', 'arremat', 'arrembent', 'arsenal', 'arteris_afl', 'arthur', 'asa', 'assalt', 'assim', 'assist', 'atend', 'atendimento', 'atentadocontraaprópriav', 'atençã', 'ating', 'atitud', 'ativ', 'atlânt', 'atrapalh', 'atravess', 'através', 'atriz', 'atropel', 'atropelado', 'atrás', 'atu', 'atã', 'automóvel', 'av', 'aven', 'aviaçã', 'avis', 'avô', 'ayrton', 'ayrtonsenn', 'azal', 'açã', 'aér', 'aí', 'babac', 'baekhyun', 'bah', 'bailacomigonov', 'bairr', 'baix', 'bal', 'balancogeralrj', 'bals', 'band', 'bandersnach', 'bandersnacht', 'bandnewsfm', 'bandnewsfmri', 'barones', 'barr', 'barra', 'bat', 'bct', 'beb', 'beij', 'bel', 'belém', 'bem', 'benfic', 'benjamin', 'beníci', 'bern', 'bertiog', 'bertioga', 'bicalh', 'bich', 'birdbox', 'birdboxnetflix', 'bizarr', 'bizarric', 'blackmirror', 'blind', 'blitz', 'bloquei', 'boat', 'boc', 'boletim', 'bolsonar', 'bom', 'bombeir', 'bomd', 'bonfim', 'bonit', 'bons', 'bonsucess', 'bost', 'botafog', 'bprv', 'br', 'br470', 'brac', 'branc', 'brasil', 'brt', 'bucet', 'bump', 'bárbara', 'bêb', 'bêbado', 'ca', 'cabec', 'cad', 'cai', 'caingangs', 'cair', 'caiu', 'caix', 'calc', 'caminh', 'caminhonet', 'caminhã', 'camp', 'campeonat', 'candelár', 'cano', 'cant', 'capac', 'capim', 'capirot', 'capital', 'capitul', 'capot', 'capotando', 'capuab', 'capuaba', 'car', 'cara', 'carapin', 'carapina', 'cardíac', 'carg', 'carl', 'carlao_barret', 'carlin', 'carlos', 'carr', 'carreir', 'carret', 'carro', 'cartun', 'carát', 'carênc', 'cas', 'casal', 'casou', 'castell', 'casual', 'catarin', 'caus', 'cax', 'cbmerj', 'cdt', 'ce', 'ceifador', 'cen', 'centr', 'central', 'centro', 'cerc', 'cet', 'cham', 'chanc', 'chanyeol', 'chapot', 'cheg', 'chei', 'choc', 'choqu', 'chor', 'chut', 'chutes', 'cidad', 'cidadã', 'cim', 'cinc', 'cinemaniac', 'cirurg', 'civil', 'cliqu', 'club', 'cois', 'colid', 'colisã', 'collegenaespn', 'coloc', 'com', 'combust', 'comec', 'comed', 'comentári', 'comercial', 'compar', 'compartilh', 'compr', 'comun', 'concord', 'concurs', 'cond', 'condomini', 'condutor', 'conf', 'confess', 'confirm', 'confissõesnamadruga2018', 'congestion', 'congonh', 'congonhas', 'conhecê', 'conosco', 'conr', 'consegu', 'consider', 'constant', 'construction', 'construção', 'cont', 'contag', 'contat', 'continu', 'continuedenunc', 'contr', 'control', 'convers', 'copacaban', 'cor', 'corp', 'corr', 'corret', 'corretíssima', 'cort', 'cost', 'cov', 'cozinh', 'cranian', 'cri', 'crianc', 'crime', 'cristianomm', 'cronistas091', 'crossfox', 'crush', 'cruzament', 'cu', 'cubucetacu', 'culp', 'culpos', 'curt', 'curv', 'când', 'cés', 'céu', 'córreg', 'côneg', 'dan', 'dant', 'dar', 'darling', 'darly', 'dd_22531177', 'de', 'decid', 'decisã', 'defeito', 'deix', 'dem', 'demais', 'demaisssss', 'denn', 'dentr', 'denunc', 'denúnc', 'deodoro', 'depois', 'der', 'derrub', 'dersasp', 'descans', 'descart', 'descobert', 'desconhecidas', 'desculp', 'desd', 'desmerec', 'dess', 'dest', 'deste', 'desvi', 'determin', 'deu', 'deus', 'dev', 'dez', 'dezembr', 'dezembro', 'dia', 'dialog', 'dias', 'dicamanaustrans', 'dicascttu', 'dig', 'diminu', 'direit', 'direçã', 'dirig', 'dirij', 'disciplin', 'dispar', 'disqu', 'distânc', 'divulg', 'diz', 'dj', 'dms', 'dobr', 'doem', 'doenc', 'doid', 'doida', 'doidã', 'dois', 'doming', 'domên', 'don', 'dr', 'duas', 'dumont', 'dupl', 'dupla', 'durant', 'dá', 'edgard', 'edmal', 'edson', 'eduard', 'efet', 'efetu', 'eldorado', 'elizabeth', 'elétr', 'em', 'embor', 'encaix', 'encaminh', 'encher', 'encontrada', 'endeus', 'enfermag', 'engar', 'enquant', 'ensai', 'entrad', 'entre', 'entreamigosfoxsports', 'entrou', 'entã', 'envi', 'envolv', 'enxurr', 'epiphany', 'equip', 'erechim', 'errad', 'erro', 'es', 'escap', 'escond', 'escrev', 'escut', 'esforc', 'eslav', 'espalh', 'especial', 'espelhodav', 'esper', 'espinh', 'espnagor', 'espos', 'esqu', 'esquin', 'estado', 'estaçã', 'estrad', 'estratég', 'estréiadaseman', 'estud', 'está', 'estádi', 'estátu', 'etern', 'eu', 'eua', 'evangeliz', 'eventual', 'evit', 'evolu', 'ex', 'exat', 'exercit', 'exist', 'expect', 'f1', 'fabian', 'fac', 'facebook', 'fagund', 'faix', 'fal', 'falabrasil', 'falagoiás', 'fals', 'falt', 'famíl', 'farmác', 'fat', 'fatal', 'faust', 'favela', 'faz', 'fech', 'feder', 'federal', 'fei', 'feir', 'feira', 'feirã', 'felipe', 'feliz', 'fer', 'feri', 'feridos', 'ferraz', 'fest', 'fez', 'fiat', 'fic', 'figueired', 'fil', 'filh', 'filha', 'film', 'final', 'finalzinh', 'fiqu', 'fiquepordentrodetud', 'fisc', 'fist', 'fit', 'flagr', 'flor', 'flu', 'fluminense', 'flux', 'fod', 'fofoqueir', 'fog', 'foiasuadenunc', 'foram', 'forc', 'forj', 'fot', 'franc', 'frances', 'francis', 'francisc', 'frança', 'fras', 'frent', 'frentist', 'friedman', 'frontal', 'frontin', 'fuck', 'fug', 'fund', 'fundã', 'funçã', 'futur', 'fácil', 'félix', 'g15', 'gad', 'gal', 'galeão', 'galudao', 'gargant', 'garotinh', 'garraf', 'gat', 'gen', 'genr', 'gent', 'genuín', 'geográf', 'georg', 'gerais', 'getúli', 'glória', 'gnewsempont', 'goalabam', 'gocg', 'godofred', 'goiân', 'gol', 'golac', 'golp', 'gom', 'gonçal', 'gost', 'govern', 'grand', 'grav', 'grave', 'gravement', 'gravíssim', 'grot', 'grotesk', 'grt', 'grup', 'gt', 'guarapari', 'guard', 'guaruj', 'guilherm', 'hahah', 'hank', 'hav', 'helicópter', 'heróis', 'hierarqu', 'hipócrit', 'histor', 'hoj', 'hom', 'homens', 'homicídi', 'hond', 'hor', 'horizonte', 'horrivel', 'hospital', 'hotel', 'hug', 'human', 'humanidade', 'humor', 'há', 'ia', 'iambyuntiful', 'ibes', 'ide', 'ident', 'identific', 'ifttt', 'ignorância', 'igrej', 'igual', 'imbiribeir', 'imedi', 'imigrantes', 'imortal', 'implant', 'import', 'importantes', 'imprevisto', 'improv', 'imprópri', 'impur', 'inclusiv', 'incrível', 'indaial', 'indic', 'individual', 'infeliz', 'infernal', 'info', 'inform', 'infraestrutur', 'infraçã', 'inic', 'intend', 'intens', 'intensamente', 'intenso', 'interdit', 'interditado', 'interdiçã', 'interess', 'interior', 'interromp', 'investig', 'iníci', 'ipir', 'ir', 'iraj', 'irmã', 'irreconhecivel', 'irregular', 'irresponsável', 'is', 'isabel', 'isabell', 'itapuã', 'ja', 'jacar', 'jackblack', 'jaguar', 'janeiro', 'jardim', 'jast', 'jeit', 'jin', 'joaquim', 'joaquinphoenix', 'jog', 'john', 'jonahhill', 'jornal', 'jos', 'jov', 'jovens', 'joã', 'juac', 'junt', 'justif', 'justific', 'jutsu', 'kavac', 'kaylan', 'kbsgayodaechukje2018', 'kenedy', 'kkkkasldksald', 'kkkkkkkk', 'kkkkkkkkkkk', 'kkkkkkklkkkkkkl', 'km', 'kms', 'koulibaly', 'la', 'lagoa', 'lagoas', 'lagos', 'laramariana05', 'laranj', 'laranja', 'laranjeir', 'lat', 'lateral', 'laud', 'laz', 'lbw', 'le', 'lei', 'leicest', 'leilã', 'lem', 'lembr', 'lembre', 'lend', 'lent', 'lentidã', 'lento', 'lev', 'liber', 'liberdad', 'liberdade', 'lig', 'likeatiger', 'lindenberg', 'linh', 'link', 'lins', 'lisbo', 'litoral', 'livars', 'livr', 'lix', 'lo', 'lob', 'loc', 'local', 'localiz', 'log', 'logic', 'long', 'longo', 'lot', 'lourenc', 'luc', 'luciandrad', 'lug', 'luiz', 'lul', 'lut', 'luís', 'líri', 'lúc', 'mach', 'machuc', 'madrug', 'madureir', 'madureiranewsrj', 'magalhães', 'maisl', 'mait', 'mal', 'mald', 'maldit', 'malh', 'malhaca', 'malu', 'man', 'manacás', 'manaus', 'manaustrans', 'manhã', 'manipul', 'mannu', 'mant', 'mantenh', 'manu', 'mar', 'maracanã', 'marc', 'marginal', 'marinh', 'marquinh', 'marty', 'maré', 'mat', 'materiais', 'mateus', 'matheus', 'maus', 'maximian', 'mc', 'med', 'mefist', 'melhor', 'men', 'mendonc', 'menin', 'menor', 'mensag', 'merc', 'merd', 'merec', 'meriti', 'mesm', 'metad', 'metro', 'metrori', 'metrô', 'meu', 'mg', 'miand', 'michael', 'mil', 'milhã', 'milit', 'mim', 'min', 'ministr', 'minut', 'miracem', 'miranda', 'mistur', 'mma', 'moacyr', 'model', 'mogi', 'molhadinhas', 'molusc', 'moment', 'momento', 'mont', 'montenegr', 'moody', 'mor', 'morador', 'morangogalinhadasortesdv', 'morr', 'morrinh', 'mort', 'mortal', 'mot', 'motel', 'motiv', 'motocicl', 'motociclet', 'motoqueir', 'motor', 'moviment', 'mt', 'mud', 'mudabrasil', 'mudari', 'muit', 'mulh', 'mult', 'mund', 'mundic', 'municipal', 'municípi', 'musical', 'má', 'mári', 'mã', 'méribel', 'músic', 'na', 'nad', 'namor', 'nao', 'naquel', 'nariz', 'natal', 'natur', 'nazar', 'neg', 'negóci', 'neiv', 'nenhum', 'ness', 'nest', 'net', 'nev', 'nfl', 'nik', 'nilópol', 'ninguém', 'niteró', 'nittrans', 'noit', 'noite', 'noooooossaaaa', 'nord', 'normal', 'normaliz', 'noronh', 'nort', 'notadefalec', 'notíc', 'notíciasbrevestg', 'nov', 'novel', 'novembr', 'novo', 'noçã', 'nun', 'nurburgring', 'nã', 'né', 'númer', 'objet', 'obrig', 'obtus', 'ocorr', 'ocorrent', 'ocup', 'oest', 'oeste', 'oficial', 'oit', 'olha', 'olhar', 'olhos', 'oliberal', 'olint', 'omit', 'onda', 'onde', 'oper', 'operacoesri', 'opost', 'oq', 'oral', 'oriente', 'orig', 'osu', 'outr', 'outubr', 'ouv', 'ouvint', 'pac', 'padilh', 'pai', 'palavr', 'palpit', 'pamonh', 'pamplon', 'par', 'paradis', 'paralel', 'paran', 'parcial', 'paro', 'parqu', 'part', 'pass', 'passarel', 'passível', 'patin', 'paul', 'paulist', 'paz', 'pc', 'ped', 'pedac', 'pedestr', 'pedidodeajud', 'peg', 'películ', 'pens', 'pequenas', 'percorr', 'perd', 'perig', 'pern', 'perpétu', 'perseguiçã', 'persever', 'personag', 'pertenc', 'pes', 'pesada', 'pesso', 'pessoal', 'phot', 'piad', 'piadasemgrac', 'pian', 'piau', 'pich', 'pilot', 'ping', 'pinheir', 'pior', 'piru', 'pist', 'pistolã', 'pixaçã', 'pl', 'plac', 'plant', 'plantã', 'plantãomistureb', 'pm', 'pmerj', 'pneu', 'pod', 'podeconfi', 'pois', 'polic', 'policial', 'políc', 'polít', 'pont', 'pordentrocomcardinot', 'porqu', 'porr', 'porrad', 'port', 'porta', 'porém', 'posit', 'possu', 'possível', 'post', 'poste', 'pouc', 'pq', 'pqp', 'pra', 'prac', 'prazo', 'prec', 'precis', 'prefeit', 'prefeitur', 'prefir', 'prejud', 'premierleaguenaespn', 'pres', 'presenc', 'president', 'prest', 'presvot', 'pret', 'prevalec', 'preven', 'previsã', 'prf', 'prf191es', 'primeir', 'princes', 'princip', 'principal', 'priscil', 'problem', 'process', 'procur', 'profet', 'proib', 'promess', 'proprietári', 'propósit', 'propõ', 'protecçã', 'protej', 'provavel', 'provident', 'provoc', 'própr', 'próxim', 'próximo', 'psic', 'pulmã', 'puls', 'pun', 'pur', 'put', 'pár', 'páti', 'pânic', 'pé', 'pólen', 'pô', 'qnd', 'quarta', 'quartadetremurasdv', 'quas', 'quatr', 'qued', 'quemavisaamigo', 'quer', 'quilômetr', 'quilômetros', 'quim', 'quint', 'quinta', 'quinz', 'quê', 'radarcostaverd', 'radardabrasil', 'radial', 'radiotransitofm', 'rafael', 'rainh', 'raiv', 'ram', 'rangon', 'rap', 'rapaz', 'ratoeir', 'rdrj', 'real', 'realeng', 'rebouc', 'recif', 'reclame', 'recompor', 'reconfortante', 'recrei', 'recuper', 'redacaosportv', 'reddit', 'reduz', 'registr', 'regiã', 'rei', 'reis', 'relacion', 'remov', 'renov', 'repit', 'repost', 'represent', 'repórt', 'resistiu', 'respeit', 'respir', 'respond', 'respons', 'result', 'ret', 'retençõ', 'retir', 'retrospectiva2018', 'retrospectivacnt', 'reveillon', 'revel', 'revolt', 'ri', 'ribeir', 'rind', 'rio', 'riodejaneir', 'rioemguerr', 'rir', 'ris', 'risc', 'rit', 'rj', 'rj106', 'rjtv', 'rn1', 'road', 'robert', 'roch', 'rodoanel', 'rodolf', 'rodov', 'rodovia', 'rodoviár', 'rodoviári', 'rol', 'romer', 'rooneym', 'ros', 'rosinh', 'rot', 'rotatór', 'roub', 'rs', 'rua', 'ruas', 'ruim', 'rum', 'réu', 'réveillon', 'sab', 'sac', 'sai', 'saib', 'sair', 'saiu', 'sampai', 'sant', 'santiag', 'saocristovaorj', 'satanás', 'saudad', 'saíd', 'saúd', 'sc', 'schumach', 'sebastiã', 'seca', 'secret', 'seg', 'segu', 'seguidor', 'segund', 'segunda', 'sei', 'seligapiloespb', 'semelh', 'sempr', 'sempre', 'semáfor', 'senador', 'send', 'senhor', 'senn', 'sensaçõ', 'sent', 'sentido', 'ser', 'serr', 'serv', 'set', 'seumelhorcaminh', 'seungcheol', 'sex', 'sext', 'sexta', 'sextadetremurasdv', 'sextaelitedosdv', 'sexy', 'shoppi', 'show', 'silv', 'sim', 'simples', 'sinistr', 'sinistro', 'sinônim', 'sit', 'situaçã', 'skat', 'smartphone', 'so', 'soar', 'sobr', 'soc', 'socorr', 'sofr', 'sol', 'sorris', 'sp', 'sp1', 'sp2', 'srivaddhanaprabh', 'standupcomedy', 'stefan', 'sub', 'success', 'sucess', 'suficient', 'suj', 'sul', 'super', 'supermerc', 'surpreend', 'surt', 'sust', 'sáb', 'sábadodetremurasdv', 'séri', 'ta', 'tagovailo', 'tailandês', 'talvez', 'tanqu', 'tant', 'taqu', 'tard', 'tav', 'tax', 'td', 'td3matheus', 'tef', 'temp', 'tempestad', 'tempor', 'temporealnews', 'tent', 'ter', 'tercadetremurasdv', 'terceir', 'termin', 'testemunh', 'tetraplég', 'the', 'tijuc', 'timborés', 'tiodopav', 'tiozã', 'tir', 'tiradent', 'tirotei', 'to', 'tod', 'todos', 'tom', 'tomb', 'tony', 'torn', 'tornando', 'torrezã', 'tosc', 'total', 'tra', 'trabalh', 'trafeg', 'tragéd', 'tralh', 'tranquil', 'trans', 'transbord', 'transform', 'transit', 'transito_rj', 'transolímp', 'transolímpica', 'transparent', 'transplant', 'tratava', 'traumat', 'travess', 'trech', 'trein', 'trem', 'trev', 'tribunal', 'tropec', 'trump', 'trás', 'trân', 'trânsit', 'trânsito', 'três', 'tud', 'tupã', 'turist', 'turm', 'twitt', 'tá', 'táx', 'tã', 'técnic', 'términ', 'têm', 'tô', 'túnel', 'ucraniano', 'ufcnocombat', 'ultimasextadoano', 'ultrapass', 'under', 'universitár', 'uno', 'uns', 'upa', 'urgent', 'urugua', 'usand', 'usar', 'usuári', 'utilitári', 'vac', 'vai', 'val', 'valdem', 'valinh', 'valoriz', 'vam', 'vamosajudaromarquinh', 'vandervalverd', 'varel', 'varg', 'vari', 'vascain', 'vaz', 'vc', 'vcnojl1', 'vcs', 'vei', 'vej', 'velh', 'veloc', 'vem', 'ver', 'verdadeir', 'vermelh', 'vest', 'vez', 'veícul', 'veículos', 'vi', 'via', 'via_040', 'viadut', 'viamã', 'vian', 'viann', 'viario', 'viasexpress', 'vic', 'vicha', 'vid', 'vida', 'vier', 'vig', 'vil', 'vinhed', 'violent', 'vir', 'visit', 'vitim', 'vitór', 'vitória', 'viv', 'vivê', 'vizinh', 'vo', 'você', 'volkswagen', 'volt', 'voluntári', 'vontad', 'vou', 'voz', 'vv', 'vár', 'vã', 'vê', 'vítim', 'vô', 'wagn', 'washington', 'xiaom', 'xiqu', 'xique', 'you', 'ze', 'zen', 'zon', 'ácid', 'árdu', 'árvor', 'édison', 'és', 'ótim', 'ônibus', 'últim']\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_TF.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "get_stop_words not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-01cf0bf2b680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_TF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: get_stop_words not found"
     ]
    }
   ],
   "source": [
    "print(.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1592)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X = tfidf_transformer.fit_transform(X_TF)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, full_data['target'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB().fit(X_train, y_train)\n",
    "svm = svm.SVC().fit(X_train,y_train)\n",
    "tree = tree.DecisionTreeClassifier().fit(X_train,y_train)\n",
    "logr = LogisticRegression().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_true,y_predicted):\n",
    "    acc = accuracy_score(y_test.values,y_predicted)\n",
    "    print('Acurácia: ' + str(acc))\n",
    "    #accuracy_score(y_true, y_predicted)\n",
    "    print('Matriz de Confusão:' )\n",
    "    print(confusion_matrix(y_true, y_predicted, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "Acurácia: 0.870588235294\n",
      "Matriz de Confusão:\n",
      "[[45  7]\n",
      " [ 4 29]]\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Acurácia: 0.823529411765\n",
      "Matriz de Confusão:\n",
      "[[45  7]\n",
      " [ 8 25]]\n",
      "\n",
      "\n",
      "Suport Vector Machines\n",
      "Acurácia: 0.611764705882\n",
      "Matriz de Confusão:\n",
      "[[52  0]\n",
      " [33  0]]\n",
      "\n",
      "\n",
      "Logistic Regressor\n",
      "Acurácia: 0.811764705882\n",
      "Matriz de Confusão:\n",
      "[[50  2]\n",
      " [14 19]]\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes')\n",
    "print_metrics(y_test,nb.predict(X_test))\n",
    "print('\\n')\n",
    "print('Decision Tree')\n",
    "print_metrics(y_test,tree.predict(X_test))\n",
    "print('\\n')\n",
    "print('Suport Vector Machines')\n",
    "print_metrics(y_test,svm.predict(X_test))\n",
    "print('\\n')\n",
    "print('Logistic Regressor')\n",
    "print_metrics(y_test,logr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
